{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_classifier_custom_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gooogr/Brain2Image/blob/master/sub_notebooks/EEG_classifier_custom_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQYla7FxIgqm"
      },
      "source": [
        "EEG signals classifier based on updated 3 paths Perceive Lab dataset<br>\n",
        "[Code](https://studentiunict-my.sharepoint.com/personal/concetto_spampinato_unict_it/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fconcetto%5Fspampinato%5Funict%5Fit%2FDocuments%2FCode%2Fonline%5Fcode%2Feeg%5Fclassification%5Fcode&originalPath=aHR0cHM6Ly9zdHVkZW50aXVuaWN0LW15LnNoYXJlcG9pbnQuY29tLzpmOi9nL3BlcnNvbmFsL2NvbmNldHRvX3NwYW1waW5hdG9fdW5pY3RfaXQvRW1TMmNVeUVXbHhOcXo5bnBpQmlwSWdCNEdnVVpMcFFJVkQ1cW9pbXBybV9YUT9ydGltZT01dXVDOThwMjJFZw)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX3L1XUaV6mC"
      },
      "source": [
        "## Imports\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from scipy.fftpack import fft, rfft, fftfreq, irfft, ifft, rfftfreq\n",
        "from scipy import signal\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n",
        "\n",
        "import torch; torch.utils.backcompat.broadcast_warning.enabled = True\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.backends.cudnn as cudnn; cudnn.benchmark = True\n",
        "from torchvision import transforms, datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#import models\n",
        "# from layers import * "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKUSg7LclwEK",
        "outputId": "7c7efe03-4d9f-4d06-ceaf-313776f5f799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 23 22:49:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg9R-CwVSY4_",
        "outputId": "db6841ea-7db0-47b0-b90f-9f1058035149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "! ln -s \"/content/drive/My Drive\" \"/content/mydrive\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50LeCBPHIa0X"
      },
      "source": [
        "Old constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KH7owS9J6xs"
      },
      "source": [
        "# DATASET_PATH = '/content/mydrive/EEG2Image_research/datasets/perceive_lab/eeg_signals_128_sequential_band_all_with_mean_std.pth'\n",
        "# SPLIT_PATH = '/content/mydrive/EEG2Image_research/datasets/perceive_lab/block_splits_by_image.pth'\n",
        "# SAVE_PATH = '/content/mydrive/EEG2Image_research/models/vanila_lstm_classifier.pth'\n",
        "# CHKPT_FOLDER = '/content/mydrive/EEG2Image_research/models/chkpt/'\n",
        "\n",
        "# FILTERING = True       # Default - True, filter raw data\n",
        "# CHECKPOINT_NAME = None # Default - None, specify checkpoint path for model training\n",
        "\n",
        "# SPLIT_NUM = 0          # Default - 0, split number\n",
        "# BATCH_SIZE = 16        # Default - 16, batch size\n",
        "# NO_CUDA = False        # Default - False, disable CUDA computation\n",
        "\n",
        "# EPOCHS = 300           # Default - 300, traning epochs number\n",
        "\n",
        "# LSTM_SIZE = 128        # Default - 128, LSTM hidden size\n",
        "# LSTM_LAYERS = 1        # Default - 1,  LSTM layers \n",
        "# EMBEDDING_SIZE = 128   # Default - 128, embedding size\n",
        "# NUM_CLASSES = 40       # Default - 40, amount of datasets classses\n",
        "# OPTIM_NAME = 'Adam'    # Default - Adam, optimizer type\n",
        "# LR = 0.001             # Default - 0.001, learning rate \n",
        "\n",
        "# LR_DECAY_FACTOR = 0.5  # Default - 0.5, learning rate decay factor, for SGD only\n",
        "# LR_DECAY_PERIOD = 10   # Default - 10  learning rate decay period, for SGD only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIIyaDfnV1_"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnajJcRpZNK"
      },
      "source": [
        "# -----Dataset options-----\n",
        "## --eeg-dataset\n",
        "EEG_DATASET = '/content/mydrive/EEG2Image_research/datasets/perceice_lab_new/eeg_55_95_std.pth'\n",
        "#EEG_DATASET = '/content/mydrive/EEG2Image_research/datasets/perceice_lab_new/eeg_5_95_std.pth'\n",
        "#EEG_DATASET = '/content/mydrive/EEG2Image_research/datasets/perceice_lab_new/eeg_14_70_std.pth'\n",
        "\n",
        "## --splits-path\n",
        "SPLIT_PATH = '/content/mydrive/EEG2Image_research/datasets/perceice_lab_new/block_splits_by_image_all.pth'\n",
        "#SPLIT_PATH = '/content/mydrive/EEG2Image_research/datasets/perceice_lab_new/block_splits_by_image_single.pth'\n",
        "\n",
        "## --split-num\n",
        "SPLIT_NUM = 0 # Default - 0, type=int, leave this always to zero.\n",
        "## --subject\n",
        "SUBJECT = 0 # Default - 0, type=int, range = [1..6] or 0 - all subjects\n",
        "\n",
        "# -----Time options-----\n",
        "##--time_low\n",
        "TIME_LOW = 20.0 # Default - 20.0, type=float, lowest time value\n",
        "##--time_high\n",
        "TIME_HIGH = 460.0 # Default - 460.0, type=float, highest time value\n",
        "\n",
        "# -----Model options-----\n",
        "##--model_type\n",
        "MODEL_TYPE = 'lstm' # Default - 'syncnet', range = ['lstm', 'EEGNet', 'syncnet']\n",
        "# - lstm is the model described in the paper \"Deep Learning Human Mind for Automated Visual Classification”, in CVPR 2017\n",
        "# - EEGNet is the model described in the paper \"EEGNet: a compact convolutional neural network for EEG-based brain–computer interfaces\", Journal of Neural Engineering 2018\n",
        "# - syncnet is the model described in the paper \"Targeting EEG/LFP synchrony with neural nets\", NIPS 2017\n",
        "\n",
        "## ????\n",
        "#parser.add_argument('-mp','--model_params', default='', nargs='*', help='list of key=value pairs of model options')\n",
        "#parser.add_argument('--pretrained_net', default='', help=\"path to pre-trained net (to continue training)\")\n",
        "\n",
        "#-----Training options-----\n",
        "##--batch_size\n",
        "BATCH_SIZE = 16 # Default - 16\n",
        "##--optim\n",
        "OPTIM = 'Adam' # Default - 'Adam'\n",
        "##--learning-rate\n",
        "LR = 0.001 # Default - 0.001\n",
        "##--learning-rate-decay-by\n",
        "LRDB = 0.5 # Default - 0.5, learning rate decay factor\n",
        "##--learning-rate-decay-every\n",
        "LRDE = 10 # Default - 10, learning rate decay period\n",
        "##--data-workers\n",
        "WORKERS = 4 # Default - 4\n",
        "##--epochs\n",
        "EPOCHS = 200 # Default - 200\n",
        "\n",
        "\n",
        "\n",
        "#-----Save options-----\n",
        "##--saveCheck\n",
        "SAVE_CHECK = 100 # Default - 100, Checkpoint period\n",
        "\n",
        "#-----Backend options-----\n",
        "NO_CUDA = False\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMmkeE601BI3"
      },
      "source": [
        "### Dataset generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ripLJhY1ECP"
      },
      "source": [
        "# Dataset class\n",
        "class EEGDataset:\n",
        "    \n",
        "    # Constructor\n",
        "    def __init__(self, eeg_signals_path):\n",
        "        # Load EEG signals\n",
        "        loaded = torch.load(eeg_signals_path)\n",
        "        if SUBJECT != 0:\n",
        "            self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==SUBJECT]\n",
        "        else:\n",
        "            self.data=loaded['dataset']        \n",
        "        self.labels = loaded[\"labels\"]\n",
        "        self.images = loaded[\"images\"]\n",
        "        \n",
        "        # Compute size\n",
        "        self.size = len(self.data)\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Process EEG\n",
        "        eeg = self.data[i][\"eeg\"].float().t()\n",
        "        eeg = eeg[TIME_LOW:TIME_HIGH,:]\n",
        "\n",
        "        # Get label\n",
        "        label = self.data[i][\"label\"]\n",
        "        # Return\n",
        "        return eeg, label\n",
        "\n",
        "# Splitter class\n",
        "class Splitter:\n",
        "\n",
        "    def __init__(self, dataset, split_path, split_num=0, split_name=\"train\"):\n",
        "        # Set EEG dataset\n",
        "        self.dataset = dataset\n",
        "        # Load split\n",
        "        loaded = torch.load(split_path)\n",
        "        self.split_idx = loaded[\"splits\"][split_num][split_name]\n",
        "        # Filter data\n",
        "        self.split_idx = [i for i in self.split_idx if 450 <= self.dataset.data[i][\"eeg\"].size(1) <= 600]\n",
        "        # Compute size\n",
        "        self.size = len(self.split_idx)\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Get sample from dataset\n",
        "        eeg, label = self.dataset[self.split_idx[i]]\n",
        "        # Return\n",
        "        return eeg, label"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VLRd-1m49ew"
      },
      "source": [
        "## Models Zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3_YhFGd5Lj7"
      },
      "source": [
        "### Base layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5_6VjlF5SPB"
      },
      "source": [
        "class ConvLayer2D(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, padding, dilation):\n",
        "        super().__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(in_channels))\n",
        "        self.add_module('relu', nn.ReLU(True))\n",
        "        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n",
        "                                          stride=stride, padding=padding, dilation=dilation, bias=True))\n",
        "        self.add_module('drop', nn.Dropout2d(0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_layers, kernel_size, stride, dilation_list, in_size):\n",
        "        super().__init__()\n",
        "        if len(dilation_list) < n_layers:\n",
        "            dilation_list = dilation_list + [dilation_list[-1]] * (n_layers - len(dilation_list))\n",
        "\n",
        "        padding = []\n",
        "        # Compute padding for each temporal layer to have a fixed size output\n",
        "        # Output size is controlled by striding to be 1 / 'striding' of the original size\n",
        "        for dilation in dilation_list:\n",
        "            filter_size = kernel_size[1] * dilation[1] - 1\n",
        "            temp_pad = math.floor((filter_size - 1) / 2) - 1 * (dilation[1] // 2 - 1)\n",
        "            padding.append((0, temp_pad))\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConvLayer2D(\n",
        "                in_channels, out_channels, kernel_size, stride, padding[i], dilation_list[i]\n",
        "            ) for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(x)\n",
        "            features.append(out)\n",
        "\n",
        "        out = torch.cat(features, 1)\n",
        "        return out\n",
        "\n",
        "class SpatialBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_spatial_layers, stride, input_height):\n",
        "        super().__init__()\n",
        "       \n",
        "        kernel_list = []\n",
        "        for i in range(num_spatial_layers):\n",
        "            kernel_list.append(((input_height // (i + 1)), 1))\n",
        "\n",
        "        padding = []\n",
        "        for kernel in kernel_list:\n",
        "            temp_pad = math.floor((kernel[0] - 1) / 2)# - 1 * (kernel[1] // 2 - 1)\n",
        "            padding.append((temp_pad, 0))\n",
        "\n",
        "        feature_height = input_height // stride[0]\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConvLayer2D(\n",
        "                in_channels, out_channels, kernel_list[i], stride, padding[i], 1\n",
        "            ) for i in range(num_spatial_layers)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(x)\n",
        "            features.append(out)\n",
        "\n",
        "        out = torch.cat(features, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETaVnL56SmI"
      },
      "source": [
        "# EEGChannelNet\n",
        "\n",
        "class FeaturesExtractor(nn.Module):\n",
        "    def __init__(self, in_channels, temp_channels, out_channels, input_width, in_height,\n",
        "                 temporal_kernel, temporal_stride, temporal_dilation_list, num_temporal_layers,\n",
        "                 num_spatial_layers, spatial_stride, num_residual_blocks, down_kernel, down_stride):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temporal_block = TemporalBlock(\n",
        "            in_channels, temp_channels, num_temporal_layers, temporal_kernel, temporal_stride, temporal_dilation_list, input_width\n",
        "        )\n",
        "\n",
        "        self.spatial_block = SpatialBlock(\n",
        "            temp_channels * num_temporal_layers, out_channels, num_spatial_layers, spatial_stride, in_height\n",
        "        )\n",
        "\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(\n",
        "                    out_channels * num_spatial_layers, out_channels * num_spatial_layers\n",
        "                ),\n",
        "                ConvLayer2D(\n",
        "                    out_channels * num_spatial_layers, out_channels * num_spatial_layers, down_kernel, down_stride, 0, 1\n",
        "                )\n",
        "            ) for i in range(num_residual_blocks)\n",
        "        ])\n",
        "\n",
        "        self.final_conv = ConvLayer2D(\n",
        "            out_channels * num_spatial_layers, out_channels, down_kernel, 1, 0, 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.temporal_block(x)\n",
        "\n",
        "        out = self.spatial_block(out)\n",
        "\n",
        "        if len(self.res_blocks) > 0:\n",
        "            for res_block in self.res_blocks:\n",
        "                out = res_block(out)\n",
        "\n",
        "        out = self.final_conv(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class Model(nn.Module):\n",
        "    '''The model for EEG classification.\n",
        "    The imput is a tensor where each row is a channel the recorded signal and each colums is a time sample.\n",
        "    The model performs different 2D to extract temporal e spatial information.\n",
        "    The output is a vector of classes where the maximum value is the predicted class.\n",
        "    Args:\n",
        "        in_channels: number of input channels\n",
        "        temp_channels: number of features of temporal block\n",
        "        out_channels: number of features before classification\n",
        "        num_classes: number possible classes\n",
        "        embedding_size: size of the embedding vector\n",
        "        input_width: width of the input tensor (necessary to compute classifier input size)\n",
        "        input_height: height of the input tensor (necessary to compute classifier input size)\n",
        "        temporal_dilation_list: list of dilations for temporal convolutions, second term must be even\n",
        "        temporal_kernel: size of the temporal kernel, second term must be even (default: (1, 32))\n",
        "        temporal_stride: size of the temporal stride, control temporal output size (default: (1, 2))\n",
        "        num_temp_layers: number of temporal block layers\n",
        "        num_spatial_layers: number of spatial layers\n",
        "        spatial_stride: size of the spatial stride\n",
        "        num_residual_blocks: the number of residual blocks\n",
        "        down_kernel: size of the bottleneck kernel\n",
        "        down_stride: size of the bottleneck stride\n",
        "        '''\n",
        "    def __init__(self, in_channels=1, temp_channels=10, out_channels=50, num_classes=40, embedding_size=1000,\n",
        "                 input_width=440, input_height=128, temporal_dilation_list=[(1,1),(1,2),(1,4),(1,8),(1,16)],\n",
        "                 temporal_kernel=(1,33), temporal_stride=(1,2),\n",
        "                 num_temp_layers=4,\n",
        "                 num_spatial_layers=4, spatial_stride=(2,1), num_residual_blocks=4, down_kernel=3, down_stride=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = FeaturesExtractor(in_channels, temp_channels, out_channels, input_width, input_height,\n",
        "                                     temporal_kernel, temporal_stride,\n",
        "                                     temporal_dilation_list, num_temp_layers,\n",
        "                                     num_spatial_layers, spatial_stride, num_residual_blocks, down_kernel, down_stride\n",
        "                                     )\n",
        "\n",
        "        encoding_size = self.encoder(torch.zeros(1, in_channels, input_height, input_width)).contiguous().view(-1).size()[0]\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(encoding_size, embedding_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(embedding_size, num_classes), \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "\n",
        "        out = out.view(x.size(0), -1)\n",
        "\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsdqOa3u54GW"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPouMkNG5-D3"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=128, lstm_size=128, lstm_layers=1, output_size=128):\n",
        "        # Call parent\n",
        "        super().__init__()\n",
        "        # Define parameters\n",
        "        self.input_size = input_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Define internal modules\n",
        "        self.lstm = nn.LSTM(input_size, lstm_size, num_layers=lstm_layers, batch_first=True)\n",
        "        self.output = nn.Linear(lstm_size, output_size)\n",
        "        self.classifier = nn.Linear(output_size,40)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Prepare LSTM initiale state\n",
        "        batch_size = x.size(0)\n",
        "        lstm_init = (torch.zeros(self.lstm_layers, batch_size, self.lstm_size), torch.zeros(self.lstm_layers, batch_size, self.lstm_size))\n",
        "        if x.is_cuda: lstm_init = (lstm_init[0].cuda(), lstm_init[0].cuda())\n",
        "        lstm_init = (Variable(lstm_init[0], volatile=x.volatile), Variable(lstm_init[1], volatile=x.volatile))\n",
        "\n",
        "        # Forward LSTM and get final state\n",
        "        x = self.lstm(x, lstm_init)[0][:,-1,:]\n",
        "        \n",
        "        # Forward output\n",
        "        x = F.relu(self.output(x))\n",
        "        x = self.classifier((x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZdsII1c5_Ou"
      },
      "source": [
        "### EEGNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KpEEDzH6cq-"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.T = 440\n",
        "        self.channels = 128\n",
        "        \n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 32), padding = 0)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
        "        \n",
        "        # Layer 2\n",
        "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
        "        self.conv2 = nn.Conv2d(97, 32, (2, 32))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(32, False)\n",
        "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
        "        \n",
        "        # Layer 3\n",
        "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv3 = nn.Conv2d(32, 64, (8, 4))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64, False)\n",
        "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # Layer 4\n",
        "        self.padding3 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv4 = nn.Conv2d(64, 128, (8, 4))\n",
        "        self.batchnorm4 = nn.BatchNorm2d(128, False)\n",
        "        self.pooling4 = nn.MaxPool2d((2, 6))\n",
        "        \n",
        "\n",
        "        # Layer 5\n",
        "        self.padding4 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv5 = nn.Conv2d(128, 256, (8, 4))\n",
        "        self.batchnorm5 = nn.BatchNorm2d(256, False)\n",
        "        self.pooling5 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # Layer 6\n",
        "        self.padding5 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv6 = nn.Conv2d(256, 512, (8, 4))\n",
        "        self.batchnorm6 = nn.BatchNorm2d(512, False)\n",
        "        self.pooling6 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # FC Layer\n",
        "        self.fc1 = nn.Linear(512, 40)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),1,x.size(1),x.size(2))\n",
        "        x = x.permute(0,1,3,2)\n",
        "        \n",
        "        # Layer 1\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.padding1(x)\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.padding2(x)\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling3(x)\n",
        "\n",
        "        # Layer 4\n",
        "        x = self.padding3(x)\n",
        "        x = F.elu(self.conv4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling4(x)\n",
        "\n",
        "        # Layer 5\n",
        "        #x = self.padding4(x)\n",
        "        #x = F.elu(self.conv5(x))\n",
        "        #x = self.batchnorm5(x)\n",
        "        #x = F.dropout(x, 0.25)\n",
        "        #x = self.pooling5(x)\n",
        "\n",
        "        # Layer 6\n",
        "        #x = self.padding5(x)\n",
        "        #x = F.elu(self.conv6(x))\n",
        "        #x = self.batchnorm6(x)\n",
        "        #x = F.dropout(x, 0.25)\n",
        "        #x = self.pooling6(x)\n",
        "\n",
        "        # FC Layer\n",
        "        x = x.view(-1, 512)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ihj_9q6c4h"
      },
      "source": [
        "### Syncnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R217rV7w6klK"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_channels=128, input_size=440, num_classes=40, num_filters=128, filter_width = 40, pool_size=40):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_filters = num_filters \n",
        "        self.num_channels = num_channels\n",
        "        self.filter_width = filter_width\n",
        "        \n",
        "        self.b = nn.Parameter(torch.FloatTensor(1,1,num_channels,num_filters).uniform_(-0.05, 0.05))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(num_filters).fill_(0))\n",
        "        self.omega = nn.Parameter(torch.FloatTensor(1,1,1,num_filters).uniform_(0, 1))\n",
        "        self.cl_Wy = int(np.ceil(float(input_size)/float(pool_size)) * num_filters)\n",
        "        \n",
        "        if(filter_width%2 == 0):\n",
        "            self.t = nn.Parameter(torch.FloatTensor(np.reshape(range(-int(filter_width/2),int(filter_width/2)),[1,int(filter_width),1,1])).fill_(0))\n",
        "        else:\n",
        "            self.t = nn.Parameter(torch.FloatTensor(np.reshape(range(-int((filter_width-1)/2),int((filter_width-1)/2) + 1),[1,int(filter_width) ,1,1])).fill_(0))\n",
        "        \n",
        "        self.phi_ini = nn.Parameter(torch.FloatTensor(1,1,num_channels, num_filters).normal_(0,0.05))\n",
        "        self.beta = nn.Parameter(torch.FloatTensor(1,1,1,num_filters).uniform_(0, 0.05))\n",
        "        \n",
        "        ## Only stride and dilation values of 1 are supported. If you use different values, padding values wont be correct\n",
        "        P = ((input_size-1)-input_size + (filter_width-1))\n",
        "        if(P%2 == 0):\n",
        "            self.padding = (P//2, P//2 + 1)\n",
        "        else:\n",
        "            self.padding = (P//2, P//2)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d((1, pool_size), stride = (1, pool_size))\n",
        "        self.classifier = nn.Linear(self.cl_Wy,num_classes)\n",
        "    \n",
        "    def forward(self,X):\n",
        "        #X must be in the form of BxCx1xT or BxCxT\n",
        "        X = X.permute(0,2,1)\n",
        "\n",
        "        self.W_osc = torch.mul(self.b,torch.cos(self.t*self.omega + self.phi_ini))\n",
        "        self.W_decay = torch.exp(-torch.pow(self.t,2)*self.beta)\n",
        "        self.W = torch.mul(self.W_osc,self.W_decay)\n",
        "        self.W = self.W.view(self.num_filters,self.num_channels,1,self.filter_width)\n",
        "        if(len(X.size()) == 3):\n",
        "            X = X.unsqueeze(2)\n",
        "    \n",
        "        res = F.conv2d(F.pad(X,self.padding,\"constant\", 0).float(),self.W.float(),bias = self.bias,stride=1 )\n",
        "        res = self.pool(F.pad(res,self.padding, \"constant\", 0))\n",
        "        res = res.view(-1,self.cl_Wy)\n",
        "        self.beta = nn.Parameter(torch.clamp(self.beta, min=0))\n",
        "        return self.classifier(F.relu(res)).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFmPtgjn28SN"
      },
      "source": [
        "## Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2QE7SVH1HCp",
        "outputId": "090cc4d6-dea6-4e9f-c41e-973e8cab599c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Load dataset\n",
        "dataset = EEGDataset(EEG_DATASET)\n",
        "# Create loaders\n",
        "loaders = {split: DataLoader(Splitter(dataset, \n",
        "                                      split_path = SPLIT_PATH, \n",
        "                                      split_num = SPLIT_NUM, \n",
        "                                      split_name = split), \n",
        "                             batch_size = BATCH_SIZE, \n",
        "                             drop_last = True, \n",
        "                             shuffle = True) for split in [\"train\", \"val\", \"test\"]}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 494 ms, sys: 3.15 s, total: 3.64 s\n",
            "Wall time: 59.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msN06qeA41_T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KN-ngdYEnAs"
      },
      "source": [
        "##########################################\n",
        "############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uXRQVb-ZLr5"
      },
      "source": [
        "# MAIN SCRIPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp2LMkfvikHB"
      },
      "source": [
        "# Load model\n",
        "\n",
        "model_options = {key: int(value) if value.isdigit() else (float(value) if value[0].isdigit() else value) for (key, value) in [x.split(\"=\") for x in opt.model_params]}\n",
        "# Create discriminator model/optimizer\n",
        "module = importlib.import_module(\"models.\" + opt.model_type)\n",
        "model = module.Model(**model_options)\n",
        "optimizer = getattr(torch.optim, opt.optim)(model.parameters(), lr = opt.learning_rate)\n",
        "    \n",
        "# Setup CUDA\n",
        "if not opt.no_cuda:\n",
        "    model.cuda()\n",
        "    print(\"Copied to CUDA\")\n",
        "\n",
        "if opt.pretrained_net != '':\n",
        "        model = torch.load(opt.pretrained_net)\n",
        "        print(model)\n",
        "\n",
        "#initialize training,validation, test losses and accuracy list\n",
        "losses_per_epoch={\"train\":[], \"val\":[],\"test\":[]}\n",
        "accuracies_per_epoch={\"train\":[],\"val\":[],\"test\":[]}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_accuracy_val = 0\n",
        "best_epoch = 0\n",
        "# Start training\n",
        "\n",
        "predicted_labels = [] \n",
        "correct_labels = []\n",
        "\n",
        "for epoch in range(1, opt.epochs+1):\n",
        "    # Initialize loss/accuracy variables\n",
        "    losses = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    accuracies = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    # Adjust learning rate for SGD\n",
        "    if opt.optim == \"SGD\":\n",
        "        lr = opt.learning_rate * (opt.learning_rate_decay_by ** (epoch // opt.learning_rate_decay_every))\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "    # Process each split\n",
        "    for split in (\"train\", \"val\", \"test\"):\n",
        "        # Set network mode\n",
        "        if split == \"train\":\n",
        "            model.train()\n",
        "            torch.set_grad_enabled(True)\n",
        "        else:\n",
        "            model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "        # Process all split batches\n",
        "        for i, (input, target) in enumerate(loaders[split]):\n",
        "            # Check CUDA\n",
        "            if not opt.no_cuda:\n",
        "                input = input.to(\"cuda\") \n",
        "                target = target.to(\"cuda\") \n",
        "            # Forward\n",
        "            output = model(input)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            losses[split] += loss.item()\n",
        "            # Compute accuracy\n",
        "            _,pred = output.data.max(1)\n",
        "            correct = pred.eq(target.data).sum().item()\n",
        "            accuracy = correct/input.data.size(0)   \n",
        "            accuracies[split] += accuracy\n",
        "            counts[split] += 1\n",
        "            # Backward and optimize\n",
        "            if split == \"train\":\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "    \n",
        "    # Print info at the end of the epoch\n",
        "    if accuracies[\"val\"]/counts[\"val\"] >= best_accuracy_val:\n",
        "        best_accuracy_val = accuracies[\"val\"]/counts[\"val\"]\n",
        "        best_accuracy = accuracies[\"test\"]/counts[\"test\"]\n",
        "        best_epoch = epoch\n",
        "    \n",
        "    TrL,TrA,VL,VA,TeL,TeA=  losses[\"train\"]/counts[\"train\"],accuracies[\"train\"]/counts[\"train\"],losses[\"val\"]/counts[\"val\"],accuracies[\"val\"]/counts[\"val\"],losses[\"test\"]/counts[\"test\"],accuracies[\"test\"]/counts[\"test\"]\n",
        "    print(\"Model: {11} - Subject {12} - Time interval: [{9}-{10}]  [{9}-{10} Hz] - Epoch {0}: TrL={1:.4f}, TrA={2:.4f}, VL={3:.4f}, VA={4:.4f}, TeL={5:.4f}, TeA={6:.4f}, TeA at max VA = {7:.4f} at epoch {8:d}\".format(epoch,\n",
        "                                                                                                         losses[\"train\"]/counts[\"train\"],\n",
        "                                                                                                         accuracies[\"train\"]/counts[\"train\"],\n",
        "                                                                                                         losses[\"val\"]/counts[\"val\"],\n",
        "                                                                                                         accuracies[\"val\"]/counts[\"val\"],\n",
        "                                                                                                         losses[\"test\"]/counts[\"test\"],\n",
        "                                                                                                         accuracies[\"test\"]/counts[\"test\"],\n",
        "                                                                                                         best_accuracy, best_epoch, opt.time_low,opt.time_high, opt.model_type,opt.subject))\n",
        "\n",
        "    losses_per_epoch['train'].append(TrL)\n",
        "    losses_per_epoch['val'].append(VL)\n",
        "    losses_per_epoch['test'].append(TeL)\n",
        "    accuracies_per_epoch['train'].append(TrA)\n",
        "    accuracies_per_epoch['val'].append(VA)\n",
        "    accuracies_per_epoch['test'].append(TeA)\n",
        "\n",
        "    if epoch%opt.saveCheck == 0:\n",
        "                torch.save(model, '%s__subject%d_epoch_%d.pth' % (opt.model_type, opt.subject,epoch))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}